\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{../common}
\usepackage{../pagesetup}
\usepackage{tikz}
% DEFINE COMMANDS IN PAGESETUP, NOT HERE

\begin{document}

\lecture{12}{October 16}{Sasha Rush}{Alexander Goldberg, Daniel Eaton, George Han, Moritz Graule}{Recurrent Neural Networks}

\subsection{Introduction}
Recall the basic structure of a time series model:
\smallskip

\begin{center}
	\begin{tikzpicture}
		\node[latent, scale=1.15]                                  (y1) {$y_{1}$};
		\node[latent, right=of y1, scale=1.15]                     (y2) {$y_{2}$};
		\node[latent, right=of y2, scale=1.15]                     (y3) {$y_{3}$};
		\node[latent, right=of y3]                                   (yt1) {$y_{t-1}$};
		\node[latent, right=of yt1, scale=1.15]                     (yt) {$y_{t}$};


		\edge [-] {y1} {y2} ;
		\edge [-] {y2} {y3} ;
 		\edge [-] {yt1} {yt} ;

		\path (y3) -- node[auto=false]{\dots} (yt1);

	\end{tikzpicture}
\end{center}

In previous lectures we discussed interpreting such a model as a UGM, with log-potentials given by:
$$\theta(y_{t}) + \theta(y_{t-1},y_{t})$$

What did we gain from this abstraction? Because all of these models have the same comditional independence structure, we are able to run the same algorithm for inference on all of these parameterizations (sum-product). Thus, conditioned on observed data, we may find the exact marginals: $p(y_{s}=v)$
\smallskip

Perhaps these structures are not necessary? Is exact inference required in cases where there is alot of data?

\subsection{RNNs}
\subsubsection{What is an RNN?}
Recall our discussion of using neural networks for classification. The UGM describing this setting is as follows:

\begin{center}
	\begin{tikzpicture}
		\node[latent, scale=1.15]                                  (y) {$y$};
		\node[latent, below=of y, xshift=-1cm, scale=1.15]      (x1) {$x_{1}$};
		\node[latent, below=of y, xshift=1cm, scale=1.15]       (xt) {$x_{T}$};

		\edge [-] {y} {x1} ;
		\edge [-] {y} {xt} ;
		\path (x1) -- node[auto=false]{\dots} (xt);

	\end{tikzpicture}
\end{center}

We could then choose to parameterize $p(y|x_{1:T})$ as a neural network:
$$p(y|x_{1:T}) = Softmax(\bold{w}^{T} \phi(x_{1:T} ; \theta))$$

where $\phi$ was just some linear combination of $x_{1:T}$ passed through a link function. If we wish to apply this scheme to cases in which $x_{1:T}$ is a \textit{sequence} we might think to use a $\phi$ of the following form:
$$\phi(x_{1:T} ; \theta) = tanh(\bold{w}\bold{x}) = tanh(\sum_{t=1}^{T} w^{(t)}x_{t})$$

However, the problem with this type of approach is that it is "time invarient," that is the same weights are shared by all of the $x_{t}$. To see why this is problematic, consider using a bag of words representation for the $X_{t}$ and encountering the two sentences: "The man ate the hot dog." and "The hot dog ate the man." While these two sentences are saying completely different things, they result in the same value generated by $\phi$.

Recurrent neural networks get around this problem by implementing the following choice of $\phi$:
$$\phi(x_{1:T} ; \theta) = tanh(\bold{w}^{(1)} x_{t} + \bold{w}^{(2)} \phi (x_{1:t-1}; \theta) + b)$$ 
where $\bold{w}^{(1)} x_{t}$ incorperates the current positional input, $\bold{w}^{(2)} \phi (x_{1:t-1}; \theta)$ carries information from the previous inputs, $b$ is the bias and $tanh$ is the chosen nonlinear transformation.

\smallskip

Representing this RNN as a computational graph:

\begin{center}
	\begin{tikzpicture}
		\node[latent, scale=0.5, label={\Large $h_{0}$}]                                  (h01) {};
		\node[latent, scale=0.5, below=of h01]                    (h02) {};
		\node[latent, scale=0.5, below=of h02]                    (h03) {};
		\node[latent, scale=0.5, right=of h01, xshift=3cm,label={\Large $h_{1}$}]                     (h11) {};
		\node[latent, scale=0.5, below=of h11]                    (h12) {};
		\node[latent, scale=0.5, below=of h12]                    (h13) {};
		\node[latent, scale=0.5, right=of h11, xshift=3cm, label={\Large $h_{2}$},label=right:{\Large{\dots}}]                     (h21) {};
		\node[latent, scale=0.5, below=of h21 ,label=right:{\Large{\dots}}]                    (h22) {};
		\node[latent, scale=0.5, below=of h22,label=right:{\Large{\dots}}]                    (h23) {};
		
		\node[latent, scale=0.5, below=of h03, xshift=1.5cm, yshift=1cm]        (x01) {};
		\node[latent, scale=0.5, right=of x01, xshift=-1.5cm, label=below:{\Large $x_{1}$}]        (x02) {};
		\node[latent, scale=0.5, right=of x02, xshift=-1.5cm]        (x03) {};
		\node[latent, scale=0.5, below=of h13, xshift=1.5cm, yshift=1cm]        (x11) {};
		\node[latent, scale=0.5, right=of x11, xshift=-1.5cm, label=below:{\Large $x_{2}$}]        (x12) {};
		\node[latent, scale=0.5, right=of x12, xshift=-1.5cm]        (x13) {};
		
		
		\edge [-] {h01} {h11,h12,h13} ;
		\edge [-] {h02} {h11,h12,h13} ;
		\edge [-] {h03} {h11,h12,h13} ;
		\edge [-] {h11} {h21,h22,h23} ;
		\edge [-] {h12} {h21,h22,h23} ;
		\edge [-] {h13} {h21,h22,h23} ;
		
		\edge [-] {x01} {h11,h12,h13} ;
		\edge [-] {x02} {h11,h12,h13} ;
		\edge [-] {x03} {h11,h12,h13} ;
		\edge [-] {x11} {h21,h22,h23} ;
		\edge [-] {x12} {h21,h22,h23} ;
		\edge [-] {x13} {h21,h22,h23} ;
% 		\edge [-] {y} {xt} ;
% 		\path (x1) -- node[auto=false]{\dots} (xt);

	\end{tikzpicture}
\end{center}

where we call $h_{t}$ the RNN hidden state. As you can see, each $h_{t}$ is a nonlinear function of $x_{1:t}$.

\subsubsection{RNN Training}

To understand how backpropagation works in RNNs, consider the functions that the NN is composed of:
$$h_{1} = tanh(\bold{w}^{(1)}\bold{x}_{1} + \bold{w}^{(2)}\bold{h}_{0} + b)$$
$$h_{2} = tanh(\bold{w}^{(1)}\bold{x}_{2} + \bold{w}^{(2)}\bold{h}_{1} + b)$$
$$\dots$$
$$h_{t} = tanh(\bold{w}^{(1)}\bold{x}_{t} + \bold{w}^{(2)}\bold{h}_{t-1} + b)$$

Notice that the only parameters to optimize in these expressions are $\bold{w}^{(1)}$ and $\bold{w}^{(2)}$, which are shared among all of the equations. In our normal representation of backpropagation we have:


\begin{center}
	\begin{tikzpicture}
	    \node[latent]                                          (box1) {};
		\node[below = of box1]                 (x1) {$x_{1}$};
		\node[right = of box1,xshift=-0.75cm,yshift=0.6cm]                 (h1) {$h_{1}$};
		\node[latent, right = of box1]                            (box2) {};
		\node[below = of box2]                 (x2) {$x_{2}$};
		\node[right = of box2,xshift=-0.75cm,yshift=0.6cm]                 (h2) {$h_{2}$};
		\node[latent, right = of box2]                            (box3) {};
		\node[below = of box3]                 (x3) {$x_{3}$};
		\node[right = of box3, xshift=-0.9cm]                 (dot) {\Large \dots};
		\node[latent, right = of box3]                            (boxt) {};
		\node[below = of boxt]                 (xt) {$x_{t}$};
		\node[right = of boxt,xshift=-0.75cm,yshift=0.6cm]                 (ht) {$h_{t}$};
		\node[right = of boxt]                            (loss) {loss};

    	\edge{x1}{box1}
    	\edge{box1}{box2}
    	\edge{x2}{box2}
    	\edge{box2}{box3}
    	\edge{x3}{box3}
    	\edge{xt}{boxt}
    	\edge{boxt}{loss}
        
        
	\end{tikzpicture}
\end{center}

Thus the size of the computational graph and the amount of backpropagation necessary will scale with the length of the input, T. Now, thinking of this situation like a simple feed-forward NN, how many layers does this network have?

\subsubsection{Issues: Network Layers}

This network will have $T$ layers, where $T$ is the length of the input, which may be very long. Thus, when performing back-propagation it is very likely that there will be problems of gradient instability -- very high (exploding) or low (vanishing) values of the gradient somewhere in the back propagation, making it hard to learn the parameters for low layers. 

\vspace{0.1in}

Consider using $\tanh$ as the activation function at each layer. Then, the gradient is close to $0$ for very large or very negative values, which is quite likely to happen somewhere in a network with many layers, so multiplying these small gradients together in back-propagation will make the contributions of the beginning of the sequence to the loss very small. Thus, it could take prohibitively long to learn weights for the beginning of the sequence. This problem is known as the problem of $\textit{vanishing gradients}$.

\subsubsection{Main idea/trick/hack for vanishing gradients}

In order to deal with vanishing gradients, we want to try to pass on more info from low layers while taking gradients, so we add connections variously called:
\begin{itemize}
    \item residual connections
    \item gated connections
    \item highway connections
    \item adaptive connections
\end{itemize}

The idea of residual connections is that we let $$\bold{h_t} = \bold{h_{t-1}} + \tanh\left(\bold{w}^{(1)} \bold{x_t} + \bold{w}^{(2)} \bold{h_{t-1}} + b\right) $$ so that taking the gradient at layer $t$ we get more information passed on from the the linear term $\bold{h}_{t-1}$ outside of the $\tanh$.

In fact, we can adaptively learn how much the gradient at each time step should be taken from the previous time step directly from the cata. Thus, we weight the contributions of the $\bold{h_t}$ and $\tanh\left(\bold{w}^{(2)} \bold{h_{t-1}} +...\right)$ by a factor $\lambda$ that is also learned from the data:
$$\bold{h_t} = \lambda \odot \bold{h}_{t-1} + (1-\lambda) \odot \tanh\left(\bold{w}^{(1)} \bold{x_t} + \bold{w}^{(2)}  \bold{h}_{t-1} + b\right)$$ 

$$\lambda = \sigma\left(\bold{w}^{(4)}h_{t-1} + \bold{w}^{(3)} \bold{x}_t + b \right)$$

By passing on information directly from previous timesteps, we can prevent vanishing gradients, since the linear terms pass on more information from previous timesteps. In this sense, the $\lambda$s function as ``memory'' of the previous timesteps. Important RNN variants using this idea are:
\begin{itemize}
    \item LSTSM (Long short-term memory networks)
    \item GRU
    \item ResNet
\end{itemize}

\subsection{Using RNNs}
\subsubsection{Classification}
Our classification algorithm has three stages:
\begin{enumerate}
    \item Run LSTM
    \item Compute Softmax   
    \item Find maximizing class
\end{enumerate}


This means that in order to make a prediction, we only need to compute 
$$ p(y_i|x_{1:T}) = \text{Softmax} (\bold{w}^{(2)} \bold{h}_i) $$
and multiply across each $y_i$ to obtain
$$ p(y_{1:T} | x_{1:T}) = \prod_{i=1}^T p(y_i|x_{1:T}) = \prod_{i=1}^T \text{Softmax} (\bold{w}^{(2)} \bold{h}_i) $$ 
Critically, this means that we do not attempt to model the relationship between the $y_i$ at all, and thus we do not need to assume any distribution over y!
\\ \\ \noindent
Lets compare this to our alternative approach, which requires full generation. Imagine that any $y_i$ is conditional on all of $y_{1:i-1}$. Then our DGM (for five nodes) is a fully connected $K_5$:

\begin{center}
\begin{tikzpicture}
  %Define nodes
  \node[latent] (y0) {$\mathbf{y_0}$};
  \node[latent, right=1cm of y0] (y1) {$\mathbf{y_1}$};
  \node[latent, right=1cm of y1] (y2) {$\mathbf{y_2}$};
  \node[latent, right=1cm of y2] (y3) {$\mathbf{y_3}$};
  \node[latent, right=1cm of y3] (y4) {$\mathbf{y_4}$};
  
  \edge{y0}{y1}
  \edge{y1}{y2}
  \edge{y2}{y3}
  \edge{y3}{y4}
  \draw [->] (y0) to [out=30,in=150] (y2);
  \draw [->] (y1) to [out=30,in=150] (y3);
  \draw [->] (y2) to [out=30,in=150] (y4);
  \draw [->] (y0) to [out=40,in=140] (y3);
  \draw [->] (y1) to [out=40,in=140] (y4);
  \draw [->] (y0) to [out=50,in=130] (y4);

\end{tikzpicture}
\end{center}

How can we then compute $p(y_s = v)$? A naive approach would be to literally enumerate all possible sequences and sum across all possibilities. However, this is very computationally expensive (exponential in $T$). Instead, we can speed up this approach by employing a greedy search. Let $$\hat{y}_1 = \text{argmax}_v \: p(y_1=v)$$
Now for each subsequent point $y_i$, we can compute
$$ \hat{y}_i = \text{argmax}_v \: p(y_1=v|\hat{y}_{i-1})$$
which is now linear in $T$, instead of exponential in $T$.

\subsubsection{Applications}
RNNs are commonly used for machine language translation and speech recognition. A simple machine translation model is that of the Encoder-Decoder model. 

\begin{center}
\begin{tikzpicture}
  %Define nodes
  \node[latent] (y1) {$\mathbf{x_1}$};
  \node[latent, right=1cm of y1] (y2) {$\mathbf{x_2}$};
  \node[latent, right=1cm of y2] (y3) {$\mathbf{x_3}$};
  \node[latent, above=1cm of y1] (h1) {$\mathbf{h_1}$};
  \node[latent, above=1cm of y2] (h2) {$\mathbf{h_2}$};
  \node[latent, above=1cm of y3] (h3) {$\mathbf{h_3}$};
  \node[latent, right=1cm of h3] (C) {$\mathbf{C}$};
  \node[latent, right=1cm of C] (h4) {$\mathbf{h_4}$};
  \node[latent, right=1cm of h4] (h5) {$\mathbf{h_5}$};
  \node[latent, right=1cm of h5] (h6) {$\mathbf{h_6}$};
  \node[latent, above=1cm of h4] (x1) {$\mathbf{y_1}$};
  \node[latent, right=1cm of x1] (x2) {$\mathbf{y_1}$};
  \node[latent, right=1cm of x2] (x3) {$\mathbf{y_2}$};
  
  \edge{y1}{h1};
  \edge{y2, h1}{h2};
  \edge{y3, h2}{h3};
  \edge{h3}{C};
  \edge{C}{h4};
  \edge{h4}{h5};
  \edge{h5}{h6};
  \edge{h4, C}{x1};
  \edge{h5, C, x1}{x2};
  \edge{h6, C, x2}{x3};

  \plate {enc} {(y1) (y2) (y3) (h1) (h2) (h3) (C)} {Encoder};
  \plate {dec} {(x1) (x2) (x3) (h4) (h5) (h6) (C)}  {Decoder};

\end{tikzpicture}
\end{center}

In this model, a sentence from the first language is fed in word by word (each $x_i$) into the Encoder. This then runs through a normal RNN setup before being fed into C, which stores the final result of the encoder. 
\\ \\ \noindent 
Now in the decoder, another RNN is run in reverse that spits out words in the second, translated, language. Each translated word  $y_i$ depends on the current layer in the decoder RNN, $h_i$, C and the last translated word (to prevent the same word from being generated multiple times). 
\begin{remark}
We will talk about \emph{Information Theory} in the next lecture.
\end{remark}
\end{document}

